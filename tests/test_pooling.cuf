!================================================================
! Test Pooling Module
!================================================================
program test_pooling
    use cudafor
    use iso_c_binding
    use pooling_cudnn
    implicit none

    type(c_ptr) :: cudnn_handle
    integer :: tests_passed = 0, tests_failed = 0

    interface
        function cudnnCreate(handle) bind(c, name='cudnnCreate')
            import :: c_ptr, c_int
            type(c_ptr), intent(out) :: handle
            integer(c_int) :: cudnnCreate
        end function
        function cudnnDestroy(handle) bind(c, name='cudnnDestroy')
            import :: c_ptr, c_int
            type(c_ptr), value :: handle
            integer(c_int) :: cudnnDestroy
        end function
    end interface

    integer :: stat

    print *, ""
    print *, "=============================================="
    print *, "  Pooling Module Test Suite"
    print *, "=============================================="
    print *, ""

    stat = cudnnCreate(cudnn_handle)
    if (stat /= 0) then
        print *, "ERROR: Failed to create cuDNN handle"
        stop 1
    endif
    print *, "cuDNN initialized"
    print *, ""

    call test_1_maxpool_dims()
    call test_2_maxpool_forward()
    call test_3_maxpool_backward()
    call test_4_upsample_forward()
    call test_5_upsample_backward()
    call test_6_pool_upsample_chain()

    print *, ""
    print *, "=============================================="
    print *, "  Test Summary"
    print *, "=============================================="
    print '(A,I2,A)', "  Passed: ", tests_passed, " tests"
    print '(A,I2,A)', "  Failed: ", tests_failed, " tests"
    print *, "=============================================="

    stat = cudnnDestroy(cudnn_handle)
    if (tests_failed > 0) stop 1

contains

    subroutine test_1_maxpool_dims()
        type(maxpool2d_layer_t) :: pool
        logical :: passed

        print *, "Test 1: MaxPool2D dimensions"

        ! 2x2 pool with stride 2: 32x32 -> 16x16
        call maxpool2d_init(pool, cudnn_handle, 2, 2, 16, 32, 32, 32)

        passed = .true.

        if (pool%out_height /= 16 .or. pool%out_width /= 16) then
            print *, "  FAIL: Expected 16x16, got", pool%out_height, "x", pool%out_width
            passed = .false.
        endif

        if (passed) then
            print *, "  PASS: 32x32 -> 16x16 with 2x2 pool"
            tests_passed = tests_passed + 1
        else
            tests_failed = tests_failed + 1
        endif

        call maxpool2d_cleanup(pool)
        print *, ""
    end subroutine

    subroutine test_2_maxpool_forward()
        type(maxpool2d_layer_t) :: pool
        real(4), device, allocatable :: input(:,:,:,:), output(:,:,:,:)
        real(4), allocatable :: h_input(:,:,:,:), h_output(:,:,:,:)
        real(4) :: expected_max
        logical :: passed
        integer :: istat

        print *, "Test 2: MaxPool2D forward"

        call maxpool2d_init(pool, cudnn_handle, 2, 2, 1, 1, 4, 4)

        allocate(input(1, 1, 4, 4))
        allocate(output(1, 1, 2, 2))
        allocate(h_input(1, 1, 4, 4))
        allocate(h_output(1, 1, 2, 2))

        ! Create known input pattern
        ! Top-left 2x2: max should be 4
        h_input(1,1,1,1) = 1.0; h_input(1,1,1,2) = 2.0
        h_input(1,1,2,1) = 3.0; h_input(1,1,2,2) = 4.0
        ! Top-right 2x2: max should be 8
        h_input(1,1,1,3) = 5.0; h_input(1,1,1,4) = 6.0
        h_input(1,1,2,3) = 7.0; h_input(1,1,2,4) = 8.0
        ! Bottom-left 2x2: max should be 12
        h_input(1,1,3,1) = 9.0; h_input(1,1,3,2) = 10.0
        h_input(1,1,4,1) = 11.0; h_input(1,1,4,2) = 12.0
        ! Bottom-right 2x2: max should be 16
        h_input(1,1,3,3) = 13.0; h_input(1,1,3,4) = 14.0
        h_input(1,1,4,3) = 15.0; h_input(1,1,4,4) = 16.0

        input = h_input

        call maxpool2d_forward(pool, input, output)
        istat = cudaDeviceSynchronize()

        h_output = output

        passed = .true.

        ! Check max pooling results
        if (abs(h_output(1,1,1,1) - 4.0) > 1e-5) then
            print *, "  FAIL: Top-left expected 4.0, got", h_output(1,1,1,1)
            passed = .false.
        endif
        if (abs(h_output(1,1,1,2) - 8.0) > 1e-5) then
            print *, "  FAIL: Top-right expected 8.0, got", h_output(1,1,1,2)
            passed = .false.
        endif
        if (abs(h_output(1,1,2,1) - 12.0) > 1e-5) then
            print *, "  FAIL: Bottom-left expected 12.0, got", h_output(1,1,2,1)
            passed = .false.
        endif
        if (abs(h_output(1,1,2,2) - 16.0) > 1e-5) then
            print *, "  FAIL: Bottom-right expected 16.0, got", h_output(1,1,2,2)
            passed = .false.
        endif

        if (passed) then
            print *, "  PASS: Max pooling produces correct max values"
            print *, "    Output: ", h_output(1,1,:,:)
            tests_passed = tests_passed + 1
        else
            tests_failed = tests_failed + 1
        endif

        deallocate(input, output, h_input, h_output)
        call maxpool2d_cleanup(pool)
        print *, ""
    end subroutine

    subroutine test_3_maxpool_backward()
        type(maxpool2d_layer_t) :: pool
        real(4), device, allocatable :: input(:,:,:,:), output(:,:,:,:)
        real(4), device, allocatable :: grad_output(:,:,:,:), grad_input(:,:,:,:)
        real(4), allocatable :: h_grad_input(:,:,:,:)
        real(4) :: grad_norm
        logical :: passed
        integer :: istat

        print *, "Test 3: MaxPool2D backward"

        call maxpool2d_init(pool, cudnn_handle, 2, 2, 16, 32, 32, 32)

        allocate(input(16, 32, 32, 32))
        allocate(output(16, 32, 16, 16))
        allocate(grad_output(16, 32, 16, 16))
        allocate(grad_input(16, 32, 32, 32))
        allocate(h_grad_input(16, 32, 32, 32))

        ! Random input
        input = 0.5
        grad_output = 0.1

        call maxpool2d_forward(pool, input, output)
        istat = cudaDeviceSynchronize()

        call maxpool2d_backward(pool, input, output, grad_output, grad_input)
        istat = cudaDeviceSynchronize()

        h_grad_input = grad_input
        grad_norm = sqrt(sum(h_grad_input**2))

        passed = .true.

        if (grad_norm < 1e-10) then
            print *, "  FAIL: Gradient is zero"
            passed = .false.
        endif

        if (grad_norm /= grad_norm) then
            print *, "  FAIL: Gradient contains NaN"
            passed = .false.
        endif

        if (passed) then
            print *, "  PASS: MaxPool backward produces valid gradients"
            print '(A,E12.4)', "    grad_input norm: ", grad_norm
            tests_passed = tests_passed + 1
        else
            tests_failed = tests_failed + 1
        endif

        deallocate(input, output, grad_output, grad_input, h_grad_input)
        call maxpool2d_cleanup(pool)
        print *, ""
    end subroutine

    subroutine test_4_upsample_forward()
        type(upsample2d_layer_t) :: up
        real(4), device, allocatable :: input(:,:,:,:), output(:,:,:,:)
        real(4), allocatable :: h_input(:,:,:,:), h_output(:,:,:,:)
        logical :: passed
        integer :: istat

        print *, "Test 4: Upsample2D forward (nearest neighbor)"

        call upsample2d_init(up, 2, 1, 1, 2, 2)

        allocate(input(1, 1, 2, 2))
        allocate(output(1, 1, 4, 4))
        allocate(h_input(1, 1, 2, 2))
        allocate(h_output(1, 1, 4, 4))

        ! Known input
        h_input(1,1,1,1) = 1.0; h_input(1,1,1,2) = 2.0
        h_input(1,1,2,1) = 3.0; h_input(1,1,2,2) = 4.0

        input = h_input

        call upsample2d_forward(up, input, output)
        istat = cudaDeviceSynchronize()

        h_output = output

        passed = .true.

        ! Top-left 2x2 should all be 1.0
        if (abs(h_output(1,1,1,1) - 1.0) > 1e-5 .or. &
            abs(h_output(1,1,1,2) - 1.0) > 1e-5 .or. &
            abs(h_output(1,1,2,1) - 1.0) > 1e-5 .or. &
            abs(h_output(1,1,2,2) - 1.0) > 1e-5) then
            print *, "  FAIL: Top-left 2x2 should be 1.0"
            passed = .false.
        endif

        ! Bottom-right 2x2 should all be 4.0
        if (abs(h_output(1,1,3,3) - 4.0) > 1e-5 .or. &
            abs(h_output(1,1,3,4) - 4.0) > 1e-5 .or. &
            abs(h_output(1,1,4,3) - 4.0) > 1e-5 .or. &
            abs(h_output(1,1,4,4) - 4.0) > 1e-5) then
            print *, "  FAIL: Bottom-right 2x2 should be 4.0"
            passed = .false.
        endif

        if (passed) then
            print *, "  PASS: Nearest neighbor upsampling correct"
            print *, "    2x2 -> 4x4"
            tests_passed = tests_passed + 1
        else
            tests_failed = tests_failed + 1
        endif

        deallocate(input, output, h_input, h_output)
        call upsample2d_cleanup(up)
        print *, ""
    end subroutine

    subroutine test_5_upsample_backward()
        type(upsample2d_layer_t) :: up
        real(4), device, allocatable :: grad_output(:,:,:,:), grad_input(:,:,:,:)
        real(4), allocatable :: h_grad_output(:,:,:,:), h_grad_input(:,:,:,:)
        logical :: passed
        integer :: istat

        print *, "Test 5: Upsample2D backward"

        call upsample2d_init(up, 2, 1, 1, 2, 2)

        allocate(grad_output(1, 1, 4, 4))
        allocate(grad_input(1, 1, 2, 2))
        allocate(h_grad_output(1, 1, 4, 4))
        allocate(h_grad_input(1, 1, 2, 2))

        ! All gradients = 1.0
        h_grad_output = 1.0
        grad_output = h_grad_output

        call upsample2d_backward(up, grad_output, grad_input)
        istat = cudaDeviceSynchronize()

        h_grad_input = grad_input

        passed = .true.

        ! Each input pixel should receive sum of 4 output gradients = 4.0
        if (abs(h_grad_input(1,1,1,1) - 4.0) > 1e-5 .or. &
            abs(h_grad_input(1,1,1,2) - 4.0) > 1e-5 .or. &
            abs(h_grad_input(1,1,2,1) - 4.0) > 1e-5 .or. &
            abs(h_grad_input(1,1,2,2) - 4.0) > 1e-5) then
            print *, "  FAIL: Each gradient should be 4.0"
            print *, "    Got:", h_grad_input(1,1,:,:)
            passed = .false.
        endif

        if (passed) then
            print *, "  PASS: Upsample backward sums gradients correctly"
            print *, "    Each pixel: 4.0 (sum of 2x2 block)"
            tests_passed = tests_passed + 1
        else
            tests_failed = tests_failed + 1
        endif

        deallocate(grad_output, grad_input, h_grad_output, h_grad_input)
        call upsample2d_cleanup(up)
        print *, ""
    end subroutine

    subroutine test_6_pool_upsample_chain()
        type(maxpool2d_layer_t) :: pool
        type(upsample2d_layer_t) :: up
        real(4), device, allocatable :: x1(:,:,:,:), x2(:,:,:,:), x3(:,:,:,:)
        real(4), allocatable :: h_x1(:,:,:,:), h_x3(:,:,:,:)
        logical :: passed
        integer :: istat

        print *, "Test 6: Pool -> Upsample chain (encoder-decoder)"

        ! Pool: 32x32 -> 16x16
        call maxpool2d_init(pool, cudnn_handle, 2, 2, 8, 64, 32, 32)
        ! Upsample: 16x16 -> 32x32
        call upsample2d_init(up, 2, 8, 64, 16, 16)

        allocate(x1(8, 64, 32, 32))
        allocate(x2(8, 64, 16, 16))
        allocate(x3(8, 64, 32, 32))
        allocate(h_x1(8, 64, 32, 32))
        allocate(h_x3(8, 64, 32, 32))

        ! Random input
        call random_number(h_x1)
        x1 = h_x1

        ! Forward: pool then upsample
        call maxpool2d_forward(pool, x1, x2)
        call upsample2d_forward(up, x2, x3)
        istat = cudaDeviceSynchronize()

        h_x3 = x3

        passed = .true.

        ! Output should be same shape as input
        if (size(h_x3, 3) /= 32 .or. size(h_x3, 4) /= 32) then
            print *, "  FAIL: Output shape mismatch"
            passed = .false.
        endif

        ! Output should not be NaN
        if (sum(h_x3) /= sum(h_x3)) then
            print *, "  FAIL: Output contains NaN"
            passed = .false.
        endif

        if (passed) then
            print *, "  PASS: Pool->Upsample preserves dimensions"
            print *, "    32x32 -> 16x16 -> 32x32"
            tests_passed = tests_passed + 1
        else
            tests_failed = tests_failed + 1
        endif

        deallocate(x1, x2, x3, h_x1, h_x3)
        call maxpool2d_cleanup(pool)
        call upsample2d_cleanup(up)
        print *, ""
    end subroutine

end program test_pooling
